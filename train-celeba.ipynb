{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install Pillow==9.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, List, Optional, Tuple, Union\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as fn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel: int = 4,\n",
    "        stride: int = 2,\n",
    "        padding: int = 1,\n",
    "        activation: str = \"leaky_relu\",\n",
    "        do_batch_norm: bool = True,\n",
    "        num_groups: int = 32,\n",
    "        negative_slope = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.GroupNorm(num_groups, in_channels) if do_batch_norm else None\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        match activation.lower():\n",
    "            case \"leaky_relu\":\n",
    "                self.act_fn = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "            case \"identity\":\n",
    "                self.act_fn = nn.Identity()\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"`activation` must be `leaky_relu` or `identity`\")\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.act_fn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        down_out_channels: Tuple[int],\n",
    "        kernels: Union[int, Tuple[int]],\n",
    "        strides: Union[int, Tuple[int]],\n",
    "        paddings: Union[int, Tuple[int]],\n",
    "        do_batch_norms: Union[bool, Tuple[bool]],\n",
    "        activations: Union[str, Tuple[str]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # check inputs\n",
    "        num_blocks = len(down_out_channels)\n",
    "        if not isinstance(kernels, int) and len(kernels) != num_blocks:\n",
    "            raise ValueError(\"`kernels` must have the same length as `down_out_channels`\")\n",
    "        if not isinstance(strides, int) and len(strides) != num_blocks:\n",
    "            raise ValueError(\"`strides` must have the same length as `down_out_channels`\")\n",
    "        if not isinstance(paddings, int) and len(paddings) != num_blocks:\n",
    "            raise ValueError(\"`paddings` must have the same length as `down_out_channels`\")\n",
    "        if not isinstance(do_batch_norms, bool) and len(do_batch_norms) != num_blocks:\n",
    "            raise ValueError(\"`do_batch_norms` must have the same length as `down_out_channels`\")\n",
    "        if not isinstance(activations, str) and len(activations) != num_blocks:\n",
    "            raise ValueError(\"`activations` must have the same length as `down_out_channels`\")\n",
    "\n",
    "        if isinstance(kernels, int):\n",
    "            kernels = (kernels,) * num_blocks\n",
    "        if isinstance(strides, int):\n",
    "            strides = (strides,) * num_blocks\n",
    "        if isinstance(paddings, int):\n",
    "            paddings = (paddings,) * num_blocks\n",
    "        if isinstance(do_batch_norms, bool):\n",
    "            do_batch_norms = (do_batch_norms,) * num_blocks\n",
    "        if isinstance(activations, str):\n",
    "            activations = (activations,) * num_blocks\n",
    "\n",
    "        self.down_blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            out_channels = down_out_channels[i]\n",
    "            self.down_blocks.append(\n",
    "                DownConv(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel=kernels[i],\n",
    "                    stride=strides[i],\n",
    "                    padding=paddings[i],\n",
    "                    activation=activations[i],\n",
    "                    do_batch_norm=do_batch_norms[i],\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.down_blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel: int = 4,\n",
    "        stride: int = 2,\n",
    "        padding: int = 1,\n",
    "        activation: str = \"relu\",\n",
    "        do_batch_norm: bool = True,\n",
    "        num_groups: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.GroupNorm(num_groups, in_channels) if do_batch_norm else None\n",
    "        self.conv = nn.ConvTranspose2d(\n",
    "            in_channels, out_channels, kernel, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        match activation.lower():\n",
    "            case \"relu\":\n",
    "                self.act_fn = nn.ReLU()\n",
    "            case \"tanh\":\n",
    "                self.act_fn = nn.Tanh()\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"`activation` must be `relu` or `tanh`\")\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.act_fn(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        up_out_channels: Tuple[int],\n",
    "        kernels: Union[int, Tuple[int]],\n",
    "        strides: Union[int, Tuple[int]],\n",
    "        paddings: Union[int, Tuple[int]],\n",
    "        do_batch_norms: Union[bool, Tuple[bool]],\n",
    "        activations: Union[str, Tuple[str]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # check inputs\n",
    "        num_blocks = len(up_out_channels)\n",
    "        if not isinstance(kernels, int) and len(kernels) != num_blocks:\n",
    "            raise ValueError(\"`kernels` must have the same length as `up_out_channels`\")\n",
    "        if not isinstance(strides, int) and len(strides) != num_blocks:\n",
    "            raise ValueError(\"`strides` must have the same length as `up_out_channels`\")\n",
    "        if not isinstance(paddings, int) and len(paddings) != num_blocks:\n",
    "            raise ValueError(\"`paddings` must have the same length as `up_out_channels`\")\n",
    "        if not isinstance(do_batch_norms, int) and len(do_batch_norms) != num_blocks:\n",
    "            raise ValueError(\"`do_batch_norms` must have the same length as `up_out_channels`\")\n",
    "        if not isinstance(activations, int) and len(activations) != num_blocks:\n",
    "            raise ValueError(\"`activations` must have the same length as `up_out_channels`\")\n",
    "\n",
    "        if isinstance(kernels, int):\n",
    "            kernels = (kernels,) * num_blocks\n",
    "        if isinstance(strides, int):\n",
    "            strides = (strides,) * num_blocks\n",
    "        if isinstance(paddings, int):\n",
    "            paddings = (paddings,) * num_blocks\n",
    "        if isinstance(do_batch_norms, int):\n",
    "            do_batch_norms = (do_batch_norms,) * num_blocks\n",
    "        if isinstance(activations, int):\n",
    "            activations = (activations,) * num_blocks\n",
    "\n",
    "        self.up_blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            out_channels = up_out_channels[i]\n",
    "            self.up_blocks.append(\n",
    "                UpConv(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel=kernels[i],\n",
    "                    stride=strides[i],\n",
    "                    padding=paddings[i],\n",
    "                    activation=activations[i],\n",
    "                    do_batch_norm=do_batch_norms[i],\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.up_blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EncoderConfig:\n",
    "    in_channels: int = 3\n",
    "    down_out_channels: Tuple[int] = (64, 128, 256, 512, 512)\n",
    "    kernels: Union[int, Tuple[int]] = 4\n",
    "    strides: Union[int, Tuple[int]] = (2, 2, 2, 2, 1)\n",
    "    paddings: Union[int, Tuple[int]] = (1, 1, 1, 1, 0)\n",
    "    do_batch_norms: Union[bool, Tuple[bool]] = (False, True, True, True, False)\n",
    "    activations: Union[str, Tuple[str]] = (\n",
    "        \"leaky_relu\",\n",
    "        \"leaky_relu\",\n",
    "        \"leaky_relu\",\n",
    "        \"leaky_relu\",\n",
    "        \"identity\",\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    in_channels: int = 512\n",
    "    up_out_channels: Tuple[int] = (512, 256, 128, 64, 3)\n",
    "    kernels: Union[int, Tuple[int]] = 4\n",
    "    strides: Union[int, Tuple[int]] = (1, 2, 2, 2, 2)\n",
    "    paddings: Union[int, Tuple[int]] = (0, 1, 1, 1, 1)\n",
    "    do_batch_norms: Union[bool, Tuple[bool]] = (True, True, True, True, False)\n",
    "    activations: Union[str, Tuple[str]] = (\n",
    "        \"relu\",\n",
    "        \"relu\",\n",
    "        \"relu\",\n",
    "        \"relu\",\n",
    "        \"tanh\",\n",
    "    )\n",
    "\n",
    "class CelebAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_config: EncoderConfig,\n",
    "        decoder_config: DecoderConfig,\n",
    "        do_init: bool = True,\n",
    "        init_mean: float = 0.0,\n",
    "        init_std: float = 0.02,\n",
    "        init_const: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if decoder_config.in_channels != encoder_config.down_out_channels[-1]:\n",
    "            raise ValueError(\n",
    "                \"`in_channels` for decoder must be the same as the last element of `down_out_channels` for encoder\"\n",
    "            )\n",
    "\n",
    "        self.encoder = Encoder(**asdict(encoder_config))\n",
    "        self.decoder = Decoder(**asdict(decoder_config))\n",
    "\n",
    "        if do_init:\n",
    "            self._initialize_params(mean=init_mean, std=init_std, const=init_const)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_params(self, mean: float, std: float, const: float):\n",
    "        def init_params(module: nn.Module):\n",
    "            if isinstance(module, Union[nn.Conv2d, nn.ConvTranspose2d]):\n",
    "                torch.nn.init.normal_(module.weight, mean=mean, std=std)\n",
    "                torch.nn.init.constant_(module.bias, val=const)\n",
    "\n",
    "        self.apply(init_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Union[str, os.PathLike],\n",
    "        resolution: int = 64,\n",
    "        center_crop: bool = True,\n",
    "        ext: str = \"jpg\",\n",
    "    ):\n",
    "        self.images = sorted(\n",
    "            [f for f in glob.glob(os.path.join(data_dir, f\"*.{ext}\"))]\n",
    "        )\n",
    "        self.pre_proc = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(resolution),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tensor:\n",
    "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        return self.pre_proc(img)\n",
    "\n",
    "def collate_fn(image_batch: List[Tensor]) -> Tensor:\n",
    "    pixel_values = torch.stack(image_batch)\n",
    "    return pixel_values.to(memory_format=torch.contiguous_format).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_means_and_stds(x: Tensor) -> Tuple[Tensor]:\n",
    "    freq = torch.fft.fft2(x)\n",
    "    real_mean = freq.real.mean(dim=0)\n",
    "    real_std = freq.real.std(dim=0)\n",
    "    imag_mean = freq.imag.mean(dim=0)\n",
    "    imag_std = freq.imag.std(dim=0)\n",
    "    return real_mean, real_std, imag_mean, imag_std\n",
    "\n",
    "def get_noise(\n",
    "    real_mean: Tensor,\n",
    "    real_std: Tensor,\n",
    "    imag_mean: Tensor,\n",
    "    imag_std: Tensor,\n",
    ") -> Tensor:\n",
    "    freq_real = torch.normal(real_mean, real_std)\n",
    "    freq_imag = torch.normal(imag_mean, imag_std)\n",
    "    freq = freq_real + 1j * freq_imag\n",
    "    noise = torch.fft.ifft2(freq)\n",
    "    return noise.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "lr = 1e-4\n",
    "adam_betas = (0.5, 0.999)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc_config = EncoderConfig()\n",
    "dec_config = DecoderConfig()\n",
    "model = CelebAModel(enc_config, dec_config, do_init=True)\n",
    "model_copy = CelebAModel(enc_config, dec_config, do_init=False).requires_grad_(False)\n",
    "model.to(device)\n",
    "model_copy.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=adam_betas)\n",
    "\n",
    "dataset = ImageDataset(\"./img_align_celeba\")\n",
    "generator = torch.Generator().manual_seed(123)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.9, 0.1], generator=generator)\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_weight = 20\n",
    "idem_weight = 20\n",
    "tight_weight = 2.5\n",
    "idem_weight /= rec_weight\n",
    "tight_weight /= rec_weight\n",
    "loss_tight_clamp_ratio = 1.5\n",
    "\n",
    "last_epoch = 0\n",
    "train_loss_hist = []\n",
    "\n",
    "checkpoint_dir = \"ign-celeba\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_name: Optional[str] = None\n",
    "if isinstance(checkpoint_name, str):\n",
    "    path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "    checkpoint = torch.load(path)\n",
    "    last_epoch = checkpoint[\"epoch\"]\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
    "    train_loss_hist = checkpoint[\"train_loss_hist\"]\n",
    "\n",
    "num_epochs = 200\n",
    "save_interval = 5\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x in train_dl:\n",
    "        bsz = x.shape[0]\n",
    "        x = x.to(device)\n",
    "        # normalize\n",
    "        x = 2.0 * x - 1.0\n",
    "\n",
    "        # get noise from input frequency statistics\n",
    "        freq_means_and_stds = get_freq_means_and_stds(x)\n",
    "        z = torch.stack([get_noise(*freq_means_and_stds) for _ in range(bsz)])\n",
    "        z = z.to(device, memory_format=torch.contiguous_format)\n",
    "\n",
    "        # compute model outputs\n",
    "        model_copy.load_state_dict(model.state_dict())\n",
    "        fx = model(x)\n",
    "        fz = model(z)\n",
    "        f_z = fz.detach()\n",
    "        ff_z = model(f_z)\n",
    "        f_fz = model_copy(fz)\n",
    "\n",
    "        # compute losses\n",
    "        loss_rec = fn.l1_loss(fx, x, reduction=\"none\").view(bsz, -1).mean(dim=-1)\n",
    "        loss_idem = fn.l1_loss(f_fz, fz, reduction=\"mean\")\n",
    "        loss_tight = -fn.l1_loss(ff_z, f_z, reduction=\"none\").view(bsz, -1).mean(dim=-1)\n",
    "        loss_tight_clamp = loss_tight_clamp_ratio * loss_rec\n",
    "        loss_tight = fn.tanh(loss_tight / loss_tight_clamp) * loss_tight_clamp\n",
    "        loss_rec = loss_rec.mean()\n",
    "        loss_tight = loss_tight.mean()\n",
    "\n",
    "        loss = loss_rec + idem_weight * loss_idem + tight_weight * loss_tight\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * bsz\n",
    "\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "    train_loss_hist.append(train_loss)\n",
    "\n",
    "    epoch = e + last_epoch + 1\n",
    "    print(f\"Epoch {epoch} loss: {train_loss:.4f}\")\n",
    "    # save checkpoint\n",
    "    if epoch % save_interval == 0 or e == num_epochs - 1:\n",
    "        path = os.path.join(checkpoint_dir, f\"epoch_{epoch}.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optim_state_dict\": optimizer.state_dict(),\n",
    "                \"train_loss_hist\": train_loss_hist,\n",
    "            },\n",
    "            path,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def generate(\n",
    "    model: CelebAModel,\n",
    "    num_steps: int,\n",
    "    sample_x: Tensor,\n",
    "    device: Any = torch.device(\"cpu\"),\n",
    ") -> List[Image.Image]:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # generate image\n",
    "    freq_means_and_stds = get_freq_means_and_stds(sample_x)\n",
    "    z = get_noise(*freq_means_and_stds).unsqueeze(0)\n",
    "    images = []\n",
    "    for _ in range(num_steps):\n",
    "        z = z.to(device)\n",
    "        z = model(z)\n",
    "        images.append(z.squeeze(0))\n",
    "\n",
    "    # denormalize\n",
    "    images = [img / 2 + 0.5 for img in images]\n",
    "    # to numpy arrays\n",
    "    images = [img.cpu().permute(1, 2, 0).float().numpy() for img in images]\n",
    "    # to PIL image\n",
    "    images = [(255 * img).round().astype(\"uint8\") for img in images]\n",
    "    images = [Image.fromarray(img) for img in images]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 50\n",
    "num_steps = 1\n",
    "for i, sample_x in enumerate(test_dl):\n",
    "    if i == num_images:\n",
    "        break\n",
    "    images = generate(model, num_steps, sample_x, device)\n",
    "    for j, img in enumerate(images):\n",
    "        img.save(f\"gen{i}_step{j+1}.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
